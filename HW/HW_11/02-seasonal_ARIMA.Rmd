# (10 points) Seasonal ARIMA model

```{r data download, results='hide'}
library(fredr)
if (fredr_has_key()){
  ecom_df <- fredr(series_id = "ECOMPCTNSA",
  observation_start = as.Date("1990-01-01"))
  ecom_df <- cbind.data.frame(ecom_df[c(1,3)], index = 1:nrow(ecom_df))
  ecom_ts <- as_tsibble(ecom_df, index = "date")
} else {
  print("Expect FREDR API key as an environment variable")
  quit(save="ask")
}
```

```{r train/test split}
split <- as.Date("2020-12-31")
train_ts <- ecom_ts %>% filter(date < split)
test_ts <- ecom_ts %>% filter(date >= split)
```
Download the series of E-Commerce Retail Sales as a Percent of Total Sales [here](https://fred.stlouisfed.org/series/ECOMPCTNSA). 

(Feel free to explore the `fredr` package and API if interested.)

Our goal is to Build a Seasonal ARIMA model, following all appropriate steps for a univariate time series model.

Separate the data set into training and test data. The training data is used to estimate model parameters, and it is for 10/1999-12/2020. The test data is used to evaluate its accuracy, and it is for 01/2021-01/22.

## Time series plot

Plot training data set of Retail Sales. What do you notice? Is there any transformation necessary?

```{r time series plot, results='hide', echo = FALSE, warning=FALSE}
#Plot the original seris
ts_plot <- autoplot(train_ts) + labs(x = element_blank(), y = 'Fraction of e-comm, %')

# Calculate the trend and de-trended series
trend <- ma(train_ts$value, order = 4, centre = T)
seasonal_comp <- train_ts$value/trend 

#Generate dataframe for modeling
decomp_df <- cbind.data.frame(train_ts, trend, seasonal_comp)
names(decomp_df) = c("my_date", "value", "index", "trend", "detrended")
decomp_df <- na.omit(decomp_df)
# Fit quadratic and logarythmic models
model.quad <- lm(formula = trend ~ index + I(index^2), data = decomp_df)
model.log <- lm(formula = log(trend) ~ index, data = decomp_df)

#Make plot for the TS
trnd_plt<- ggplot(data = decomp_df) + 
  geom_line(aes(x = my_date, y = trend )) + 
  scale_y_continuous(expand = c(0,0)) + labs(x = element_blank())

#Make plot for the model residuals
res_plt<- ggplot(data = decomp_df) + 
  geom_line(aes(x = my_date, y = model.quad$residuals, color = "Quadratic")) + 
  geom_line(aes(x = my_date, y = model.log$residuals, color = "Log" )) + 
  scale_y_continuous(expand = c(0,0)) + labs(x = element_blank(), y = "Model residuals") +
  scale_color_manual(name = "Trend Models", values = c("Quadratic" = "black", 
                                           "Log" = "blue")) +
  # Legend text and position
  theme(legend.position=c(.3,.75))
# Make plot for the de-trended series
seasonal_plt <- ggplot(data = decomp_df) + aes(x = my_date, y = detrended ) + geom_line() + 
  scale_y_continuous(expand = c(0,0)) + labs(x = element_blank())

#Display plots
ts_plot/(trnd_plt|res_plt|seasonal_plt)
```
Fraction of e-commerce in the overall retail sales is growing, and its growth is accelerating. It appears that at around the start of the COVID-19 pandemic the acceleration increased. This is evident from abrupt change in the overall trend curvature at the beginning of 2020. The series also has marked seasonal component, and the magnitude of the seasonal component is increasing, suggesting a multiplicative seasoning. The trend has obvious curvature, but neither quadratic, nor logarithmic transformation result in white-noise residuals.

## Check for Stationary 

Use ACF/PACF and a unit root test to check if Retail Sales is stationary. If data is not stationary, difference the data, and apply the
test again until it becomes stationary? How many differences are needed to make data stationary?

```{r difference the series, warning=FALSE}
my_lag <- 4
train_ts <- mutate(train_ts, 
                   deseasoned = difference(value, lag = my_lag),
                   detrended = difference(deseasoned, lag = 1))
train_ts <- na.omit(train_ts)

deseasoned_plot <- ggplot(data = train_ts) + aes(x = date, y = deseasoned) + 
                  geom_line()
detrended_plot <- ggplot(data = train_ts) + aes(x = date, y = detrended) +
                  geom_line()
deseasoned_plot | detrended_plot 
```

```{r Check for Stationary}
value_p <- PP.test(train_ts$value)
deseasoned_p <- PP.test(train_ts$deseasoned)
detrended_p <- PP.test(train_ts$detrended)
```
Given strong trend and seasonality in the data it is obvious that the original series is not stationary. Deseasonng via differencing with lag 4 (1 year), does not eliminate the trend. Further de-trending with lag = 1 eliminates the trend, but the resulting trend is clearly heteroschedastic. Results of Phillips-Perron test reflect these observations. The test fails to reject the hypothesis of non-stationarity for the original series (p-value = `r round (value_p$p.value, digits = 3)`) and de-seasoned series (p-value = `r round(deseasoned_p$p.value, digits = 3)`), but strongly rejects this hypothesis for the de-trended series with p-value = `r round(detrended_p$p.value, digits = 3)`)

## Model identification and estimation

Use ACF/PACF to identify an appropriate SARIMA model. Estimate both select model and model chosen by ARIMA()

```{r Model identification}
acf_plot <- acf(train_ts$detrended, plot = F) %>% autoplot() + xlim(1,25)
pacf_plot <- pacf(train_ts$detrended, plot = F) %>% autoplot() + xlim(1,25)
acf_plot | pacf_plot
```
```{r Manual Model estimation}
model.manual<- arima(train_ts$value, order = c(1, 1, 1),
  seasonal = list(order = c(0, 1, 0), period = 4))
model.manual
```
```{r auto model estimation}
model.auto <- train_ts$value %>% ts(frequency = 4) %>% auto.arima(d = 1, D = 1,
              max.p = 5, max.q = 5, max.P = 2, max.Q = 2, max.d = 2, max.D = 2,
              max.order = 10,
              start.p = 0, start.q = 0, start.P = 0, start.Q = 0,
  ic="aic", seasonal = TRUE, stepwise = FALSE, approximation = FALSE, trace = FALSE)
model.auto

train_short <- train_ts %>% filter(date < as.Date("2020-01-01"))
model.short <- train_short$value %>% ts(frequency = 4) %>% auto.arima(d = 1, D = 1,
              max.p = 5, max.q = 5, max.P = 2, max.Q = 2, max.d = 2, max.D = 2,
              max.order = 10,
              start.p = 0, start.q = 0, start.P = 0, start.Q = 0,
  ic="aic", seasonal = TRUE, stepwise = FALSE, approximation = FALSE, trace = FALSE)
model.short
```

ACF plot for de-seasoned and de-trended series contains one strongly significant lag = 1, same as PACF plot. Formally, this is typical for ARMA(1.1) process, however, there is also a chance that this is an artifact caused by the few lags at the end of the series that are affected by the pandemic abnormality. Having abnormally large values at the tail of the series might make short lags a lot more significant. This considerations aside, ACF and PACF plot suggest SARIMA(1.1.1)(0.1.0)[4] model. Estimating this model results in AIC = `r round(model.manual$aic, digits = 1)`.
Grid search with `auto.arima` function yields very similar result. The function find s the lowest AIC `r round(model.auto$aic, digits = 1)` for SARIMA(1.1.0)(0.1.0)[4]. The fact that MA component of ARIMA does not improve the model, despite the fact that PACF plot goes down abruptly after lag 1, supports the hypothesis that observed plots are just artifacts.
Repeating this grid search on the shortned training set that excludes the pandemic data results in a much lower AIC of `r round(model.short$aic, digits = 1)`.

## Model diagnostic 

Do residual diagnostic checking of both models. Are the residuals white noise? Use the Ljung-box test to check if the residuals are white noise.  

```{r diagnostic for manual model}
checkresiduals(model.manual, test=FALSE, plot = TRUE)
```

```{r diagnostic for auto model}
checkresiduals(model.auto, test=FALSE, plot = TRUE)
```

```{r diagnostic for short-set model}
checkresiduals(model.short, test=FALSE, plot = TRUE)
```

> 'Fill this in


## Forcasting 

Use the both models to forecast the next 12 months and evaluate the forecast accuracy of these models.

```{r forcasting}
#Fill this in

```

> 'Fill this in